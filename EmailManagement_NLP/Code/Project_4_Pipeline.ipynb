{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for NLP \n",
    "\n",
    "### Includes: \n",
    "\n",
    "#### * Text Preprocessing (NLTK, SpaCy, Textblob) \n",
    "#### * Vectorization (Count Vectorizer, TF-IDF Vectorizer) \n",
    "#### * Topic Modeling (LSA, LDA, NMF) \n",
    "#### * Dimensionality Reduction (SVD, PCA, tSNE) \n",
    "#### * Clustering (K-Means, Agglomerative, DBSCAN, Mean Shift, Spectral) \n",
    "#### * Visualizations in 3D & 2D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import porter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.en import STOP_WORDS\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "%matplotlib inline\n",
    "# plt.style.use('seaborn')\n",
    "plt.style.use('seaborn-white')\n",
    "# plt.style.use(['dark_background', 'presentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:14:59) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] \n",
      "\n",
      "Matplotlib Version: 2.0.2\n",
      "Numpy Version: 1.12.1\n",
      "Pandas Version: 0.20.3\n",
      "NLTK Version: 3.2.4\n"
     ]
    }
   ],
   "source": [
    "libraries = (('Matplotlib', matplotlib), ('Numpy', np), ('Pandas', pd), ('NLTK', nltk))\n",
    "\n",
    "print(\"Python Version:\", sys.version, '\\n')\n",
    "for lib in libraries:\n",
    "    print('{0} Version: {1}'.format(lib[0], lib[1].__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data & MongoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Increase the field_size_limit\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CSV to Json\n",
    "csvfile = open(\"../../Data/emails.csv\", 'r')\n",
    "reader = csv.DictReader(csvfile)\n",
    "\n",
    "# Pandas DataFrame to Json\n",
    "reader = json.loads(dataframe_name.T.to_json()).values()\n",
    "\n",
    "# Connect to MongoDB \n",
    "client = MongoClient()\n",
    "db = client.database_name\n",
    "\n",
    "# Drop Json to MongoDB \n",
    "db.collection_name.drop()\n",
    "header=[\"col1\",\"col2\", \"col3\",\"col4\",\"col5\",\"col6\",\"col7\"]\n",
    "\n",
    "for each in reader: \n",
    "    row = {}\n",
    "    for field in header: \n",
    "        row[field] = each[field]\n",
    "    db.collection_name.insert(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data from Mongo \n",
    "client = MongoClient()\n",
    "db = client.database_name\n",
    "cursor = db.collection_name.find({})\n",
    "\n",
    "# Construct the DataFrame\n",
    "df =  pd.DataFrame(list(cursor))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email Info Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_text_from_email(email_):\n",
    "    msg = email.message_from_string(email_)\n",
    "    for content in msg.walk():\n",
    "        if content.get_content_type() == 'text/plain':\n",
    "            return content.get_payload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages = list(map(email.message_from_string, df['message']))\n",
    "\n",
    "# Sets keys that are commonly used by most messages \n",
    "keys = messages[9].keys()\n",
    "print(keys)\n",
    "\n",
    "# Add keys and corresponding info to DataFrame \n",
    "for key in keys:\n",
    "    df[key] = [doc[key] for doc in messages]\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP - Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK \n",
    "def nltk_tokenizer_(raw_string):\n",
    "    \"\"\" Function that returns cleaned, tokenized text.\n",
    "    Input: \n",
    "        (str) raw text (corpus of documents)\n",
    "    Text Preprocessing: \n",
    "        1. Removal of numbers, punctuations and special charaters\n",
    "        2. Lowercase-conversion \n",
    "        2. Tokenization of text \n",
    "        3. Lemmatization & Stemming \n",
    "        5. Removal of stop words\n",
    "        6. Removel of small words (length less than 3)\n",
    "    Output: \n",
    "       cleaned text (tokens).\n",
    "    \"\"\"\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    lemmizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "    stop += ['.',',','(', ')',\"'\",'\"']\n",
    "    stop = set(stop) \n",
    "    \n",
    "    # lower all letter \n",
    "    raw_string = raw_string.lower()\n",
    "    # get rid of numbers \n",
    "    raw_string = raw_string.translate(str.maketrans('','','1234567890'))\n",
    "    # tokenize text with RegexpTokenizer to get rid of punctuations\n",
    "    tokens = tokenizer.tokenize(raw_string)\n",
    "    # lemmatize \n",
    "    tokens = [lemmatizer.lemmatize(i) for i in tokens]\n",
    "    # stemming \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "    # get rid of stop words \n",
    "    tokens = [i for i in tokens if i not in stop]\n",
    "    # get rid of small words \n",
    "    tokens = [i for i in tokens if len(i)>3]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SpaCy \n",
    "def spacy_tokenizer_(raw_string):\n",
    "    \"\"\" Function that returns cleaned, tokenized text.\n",
    "    Input: \n",
    "        (str) raw text (corpus of documents)\n",
    "    Text Preprocessing: \n",
    "        1. Removal of numbers, punctuations and special charaters\n",
    "        2. Lowercase-conversion \n",
    "        2. Tokenization of text \n",
    "        3. Lemmatization & Stemming \n",
    "        5. Removal of stop words\n",
    "        6. Removel of small words (length less than 3)\n",
    "    Output: \n",
    "       cleaned text (tokens).\n",
    "    \"\"\"\n",
    "    nlp = en_core_web_sm.load()\n",
    "    punctuations = string.punctuation + '.doc'\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "    raw_string = re.sub(r'\\-\\-+', '', raw_string)\n",
    "    raw_string = raw_string.translate(str.maketrans('','','1234567890'))\n",
    "    \n",
    "    tokens = nlp(raw_string)\n",
    "    tokens = [tok.lemma_.lower().strip() \n",
    "              if tok.lemma_ != \"-PRON-\" \n",
    "              else tok.lower_ for tok in tokens]\n",
    "    tokens = [tok for tok in tokens \n",
    "              if (tok not in stopwords and tok not in punctuations)]   \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "    tokens = [i for i in tokens if len(i)>3]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TextBlob \n",
    "def textblob_tokenizer_(raw_string):\n",
    "    \"\"\" Function that returns cleaned, tokenized text.\n",
    "    Input: \n",
    "        (str) raw text (corpus of documents)\n",
    "    Text Preprocessing: \n",
    "        1. Removal of numbers, punctuations and special charaters\n",
    "        2. Lowercase-conversion \n",
    "        2. Tokenization of text \n",
    "        3. Lemmatization & Stemming \n",
    "        5. Removal of stop words\n",
    "        6. Removel of small words (length less than 3)\n",
    "    Output: \n",
    "       cleaned text (tokens).\n",
    "    \"\"\"\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    lemmizer = WordNetLemmatizer()\n",
    "    \n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "    stop += ['.',',','(', ')',\"'\",'\"']\n",
    "    stop = set(stop)\n",
    "    \n",
    "    raw_string = raw_string.lower()\n",
    "    raw_string = re.sub(' [A-Z]* ', '', raw_string)\n",
    "    raw_string = raw_string.translate(str.maketrans('','','1234567890'))\n",
    "    \n",
    "    tokens = TextBlob(raw_string).words\n",
    "    tokens = [w for w in tokens if w not in stop]\n",
    "    tokens = [lemmatizer.lemmatize(i) for i in tokens]\n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "    tokens = [i for i in tokens if len(i)>3]\n",
    "                                      \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP - Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer & TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count Vectorizer \n",
    "def count_vectorizer(tokenizer, \n",
    "                     min_n=1, \n",
    "                     max_n=2, \n",
    "                     max_features=1000, \n",
    "                     max_df=0.6):\n",
    "    \"\"\" This function returns a count vectorizer.\n",
    "    Input: \n",
    "        1. tokenizer: tokenizer to tokenize and cleanse the text \n",
    "        2. min_n: lower boundary of n-values for \n",
    "                  different n-grams to be extracted\n",
    "        3. max_n: upper boundary of n-values for \n",
    "                  different n-grams to be extracted\n",
    "        4. max_features: max features returned with top term frequency \n",
    "        5. max_df: the max document frequency allowed for a single word\n",
    "    Output: \n",
    "        Vectorized text\n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer = CountVectorizer(tokenizer=tokenizer,\n",
    "                                 ngram_range=(min_n,max_n),\n",
    "#                                stop_words='english', \n",
    "#                                token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "#                                lowercase=True\n",
    "                                 max_features=max_features,\n",
    "                                 max_df = max_df) \n",
    "    \n",
    "    vect_data = vectorizer.fit_transform(data)\n",
    "    norm_data = Normalizer().fit_transform(vect_data)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    return norm_data, feature_names\n",
    "\n",
    "# TF-IDF Vectorizer     \n",
    "def tfidf_vectorizer(tokenizer, \n",
    "                     min_n=1, \n",
    "                     max_n=2, \n",
    "                     max_features=1000, \n",
    "                     max_df=0.6): \n",
    "    \"\"\" This function returns a TF-IDF vectorizer.\n",
    "    Input: \n",
    "        1. tokenizer: tokenizer to tokenize and cleanse the text \n",
    "        2. min_n: lower boundary of n-values for \n",
    "                  different n-grams to be extracted\n",
    "        3. max_n: upper boundary of n-values for \n",
    "                  different n-grams to be extracted\n",
    "        4. max_features: max features returned with top term frequency \n",
    "        5. max_df: the max document frequency allowed for a single word\n",
    "    Output: \n",
    "        Vectorized text\n",
    "    \"\"\"\n",
    "\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenizer,\n",
    "                                 ngram_range=(min_n, max_n),  \n",
    "#                                stop_words='english', \n",
    "#                                token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "#                                lowercase=True,\n",
    "                                 max_features=max_features,\n",
    "                                 max_df = 0.6)\n",
    "    \n",
    "    vect_data = vectorizer.fit_transform(data)\n",
    "    norm_data = Normalizer().fit_transform(vect_data)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    return norm_data, feature_names    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizeration + DataFrame Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count Vectorizer \n",
    "def count_vectorizer_df(content, \n",
    "                        tokenizer, \n",
    "                        min_n, \n",
    "                        max_n, \n",
    "                        max_features=1000, \n",
    "                        max_df=0.6):  \n",
    "    \"\"\" This function returns a vectorized pandas dataframe \n",
    "    with count vectorizer.\n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer = CountVectorizer(tokenizer=tokenizer, \n",
    "                                 ngram_range=(min_n,max_n),\n",
    "#                                  stop_words='english', \n",
    "#                                  token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "#                                  lowercase=True\n",
    "                                 max_features=max_features,\n",
    "                                 max_df = max_df) \n",
    "    vec_text = vectorizer.fit_transform(content)    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    df = pd.DataFrame(vec_text.toarray(), columns=feature_names)\n",
    "    return df \n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "def tfidf_vectorizer_df(content, \n",
    "                        tokenizer, \n",
    "                        min_n, \n",
    "                        max_n, \n",
    "                        max_features=1000, \n",
    "                        max_df=0.6):     \n",
    "        \"\"\" This function returns a vectorized pandas dataframe \n",
    "        with TF-IDF vectorizer.\n",
    "        \"\"\"\n",
    "        \n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenizer, \n",
    "                                 ngram_range=(min_n,max_n),\n",
    "#                                  stop_words='english', \n",
    "#                                  token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "#                                  lowercase=True\n",
    "                                 max_features=max_features,\n",
    "                                 max_df = max_df) \n",
    "    vec_text = vectorizer.fit_transform(content)    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    df = pd.DataFrame(vec_text.toarray(), columns=feature_names)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    \"\"\" Returns a list of topics identified \n",
    "    by different topic modeling algorithms.\n",
    "    \"\"\"\n",
    "    \n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "def display_topics_lda(model, feature_names, no_top_words, d):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        print(\"Topic \", ix, \"Score\", d[ix])\n",
    "        print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-no_top_words - 1:-1]]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDA (Latent Dirichlet Allocation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_algo(vectorized_data, \n",
    "             n_comp=10, \n",
    "             n_iter=10, \n",
    "             random_state=7777, \n",
    "             learning_method='online',\n",
    "             n_jobs=-1):\n",
    "    \"\"\" Returns a topic modeling model employing LDA algorithm\n",
    "    with specified inputs and transformed data.\n",
    "    Input: \n",
    "        1. vectorized data \n",
    "        2. n_comp: number of topics to extract from data\n",
    "    Output: \n",
    "        1. LDA model \n",
    "        2. Data that's been trained and transformed\n",
    "        \"\"\"\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_comp,\n",
    "                                max_iter=n_iter,\n",
    "                                random_state=random_state,\n",
    "                               learning_method=learning_method)\n",
    "    \n",
    "    data = lda.fit_transform(vectorized_data)\n",
    "    scor = data[0]\n",
    "\n",
    "    return lda, data, scor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF (Non-Negative Matrix Factorization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nmf_algo(vectorized_data, n_comp=10, random_state=7777,n_jobs=-1):\n",
    "    \"\"\" Returns a topic modeling model employing NMF algorithm\n",
    "    with specified inputs and transformed data.\n",
    "    Input: \n",
    "        1. vectorized data \n",
    "        2. n_comp: number of topics to extract from data\n",
    "    Output: \n",
    "        1. NMF model \n",
    "        2. Data that's been trained and transformed\n",
    "        \"\"\"\n",
    "    \n",
    "    nmf = NMF(n_components=n_comp)\n",
    "    data = nmf.fit_transform(vectorized_data)\n",
    "    scor = data[0]\n",
    "    \n",
    "    return nmf, data, scor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA (Latent semantic analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lsa_algo(vectorized_data, n_comp=10, random_state=7777,n_jobs=-1):\n",
    "    \"\"\" Returns a topic modeling model employing NMF algorithm\n",
    "    with specified inputs and transformed data.\n",
    "    Input: \n",
    "        1. vectorized data \n",
    "        2. n_comp: number of topics to extract from data\n",
    "    Output: \n",
    "        1. NMF model \n",
    "        2. Data that's been trained and transformed\n",
    "    \"\"\"\n",
    "    lsa = TruncatedSVD(n_components=n_comp)\n",
    "    data = lsa.fit_transform(vectorized_data)\n",
    "    scor = data[0]\n",
    "    \n",
    "    return lsa, data, scor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display_topics(lsa_tfidf,count_vectorizer.get_feature_names(),10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA \n",
    "* Improve clustering\n",
    "* Improve classification (alternative to feature selection)\n",
    "* Visualize high dimensional data in 2D or 3D \n",
    "* Data compression with little loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_clusters_3D_pca(data, n_comp=3, random_state=7777):\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    reduced_data = pca.fit_transform(data)\n",
    "\n",
    "    fig = plt.figure(1, figsize=(8, 6))\n",
    "    ax = Axes3D(fig, elev=-160, azim=130)\n",
    "    ax.scatter(reduced_data[:, 0],\n",
    "               reduced_data[:, 1], \n",
    "               reduced_data[:, 2], \n",
    "#                c=y, \n",
    "               cmap=plt.cm.Paired)\n",
    "    ax.set_title(\"Enron Emails in 3D\")\n",
    "    ax.set_xlabel(\"sepal length\")\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.set_ylabel(\"sepal width\")\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.set_zlabel(\"petal length\")\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "    plt.show()  \n",
    "    \n",
    "    return pca, reduced_data\n",
    "\n",
    "def plot_3D(X):\n",
    "    x,y,z = zip(*X)\n",
    "    plt.style.use(\"seaborn-poster\")\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_clusters_3D_svd(data, n_comp=3, random_state=7777):\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=n_comp, random_state=random_state)\n",
    "    reduced_data = svd.fit_transform(data)\n",
    "        \n",
    "    fig = plt.figure(1, figsize=(8, 6))\n",
    "    ax = Axes3D(fig, elev=-160, azim=130)\n",
    "    ax.scatter(reduced_data[:, 0],\n",
    "               reduced_data[:, 1], \n",
    "               reduced_data[:, 2], cmap='Set1')\n",
    "    \n",
    "    ax.set_title(\"Enron Emails in 3D\")\n",
    "    ax.set_xlabel(\"topic1\")\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.set_ylabel(\"topic2\")\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.set_zlabel(\"topic3\")\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "    plt.show() \n",
    "    \n",
    "    return svd, reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_clusters_2D_svd(data, n_comp=2, random_state=7777):\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=n_comp, random_state=random_state)\n",
    "    reduced_data = svd.fit_transform(data)\n",
    "        \n",
    "    fig = plt.figure(1, figsize=(8, 6))\n",
    "#     ax = Axes3D(fig, elev=-160, azim=130)\n",
    "    plt.scatter(reduced_data[:, 0],\n",
    "               reduced_data[:, 1])\n",
    "#                cmap='Set1')\n",
    "    \n",
    "#     ax.set_title(\"Enron Emails in 3D\")\n",
    "#     ax.set_xlabel(\"topic1\")\n",
    "#     ax.w_xaxis.set_ticklabels([])\n",
    "#     ax.set_ylabel(\"topic2\")\n",
    "#     ax.w_yaxis.set_ticklabels([])\n",
    "\n",
    "    plt.show() \n",
    "    \n",
    "#     return svd, reduced_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "* For Visualization of High Dim Data Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_high_dim_data_tsne(data, num_clust=10, \n",
    "                            perplexity=30, \n",
    "                            num_points=100, \n",
    "                            n_comp=2, \n",
    "                            learning_rate=200, \n",
    "                            random_state=7777):\n",
    "    tsne = TSNE(n_components=n_comp, \n",
    "                perplexity=perplexity, \n",
    "                learning_rate=learning_rate, \n",
    "                random_state=random_state, verbose=2)\n",
    "    \n",
    "    low_data = tsne.fit_transform(data)\n",
    "\n",
    "    colorize = []\n",
    "    \n",
    "    for i in range(num_clust):\n",
    "        for _ in range(num_points):\n",
    "            colorize.append(plt.cm.rainbow(i*20))\n",
    "            \n",
    "    x,y = zip(*low_data)\n",
    "    \n",
    "    plt.scatter(x,y,c=colorize,s=40)\n",
    "    \n",
    "#     return tsne, low_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standardized_data = StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering \n",
    "https://github.com/thisismetis/chi17_ds4/blob/master/class_lectures/week07-fletcher1/01-unsup_kmeans/simple_kmeans_demo.ipynb\n",
    "\n",
    "https://github.com/dziganto/Data_Science_Fundamentals/blob/master/notebooks/Machine_Learning/Kmeans_Clustering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_means_clustering(X, min_k, max_k, random_state=7777):\n",
    "    SSEs = []\n",
    "    Sil_coefs = []\n",
    "    for k in range(min_k, max_k):   \n",
    "        km = KMeans(n_clusters=k, random_state=random_state)\n",
    "        clusters = km.fit_predict(X)\n",
    "        labels = km.labels_ \n",
    "        Sil_coefs.append(silhouette_score(X, labels, metric='euclidean'))\n",
    "        SSEs.append(km.inertia_)\n",
    "        \n",
    "        \n",
    "    return clusters, Sil_coefs, SSEs \n",
    "\n",
    "def plot_k_means(X, n_clust=5): \n",
    "    km = KMeans(n_clusters=n_clust)\n",
    "    clusters = km.fit_predict(X)\n",
    "    x,y = zip(*X)\n",
    "    plt.figure(dpi=200)\n",
    "    plt.scatter(X[:,0],X[:,1],c=plt.cm.rainbow(clusters*20),s=14);\n",
    "\n",
    "def plot_kmeans_evaluation(Sil_coefs, SSEs, min_k, max_k):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,5), \n",
    "                                   sharex=True, dpi=200)\n",
    "    k_clusters = range(min_k, max_k)\n",
    "    ax1.plot(k_clusters, Sil_coefs)\n",
    "    ax1.set_xlabel('number of clusters')\n",
    "    ax1.set_ylabel('silhouette coefficient')\n",
    "\n",
    "    ax2.plot(k_clusters, SSEs)\n",
    "    ax2.set_xlabel('number of clusters')\n",
    "    ax2.set_ylabel('SSE');\n",
    "    \n",
    "def plot_silhoutte(X, min_k, max_k, random_state=7777):\n",
    "    for k in range(min_k, max_k):\n",
    "        plt.figure(dpi=150, figsize=(8,6))\n",
    "        ax1 = plt.gca()\n",
    "        km = KMeans(n_clusters=k, random_state=random_state)\n",
    "        km.fit(X)\n",
    "        labels = km.labels_\n",
    "        silhouette_avg = silhouette_score(X, labels)\n",
    "        print(\"For n_clusters =\", k,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(X, labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(k):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = plt.cm.spectral(float(i) / k)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster \n",
    "            # numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN Clustering \n",
    "http://localhost:8888/notebooks/metisgh/chi17_ds4/class_lectures/week07-fletcher1/05-more_clustering/other_clustering_sklearn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dbscan_clustering(X, eps, minpoints):\n",
    "    \"\"\" A function that employs DBSCAN clustering to cluster text data\n",
    "    and plots clusters.\n",
    "    Input: \n",
    "        X: Standardized data for best results\n",
    "    \"\"\"\n",
    "    \n",
    "    dbscan = DBSCAN(eps=eps, min_samples=minpoints).fit(X)\n",
    "    labels = dbscan.labels_\n",
    "    unique_labels = set(labels)\n",
    "    \n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "    plt.figure(dpi=200)\n",
    "    show_core = True\n",
    "    show_non_core = True\n",
    "\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = 'k'\n",
    "        \n",
    "        class_member_mask = (labels == k)\n",
    "        \n",
    "        if show_core:\n",
    "            xy = X[class_member_mask & core_samples_mask]\n",
    "            x, y = xy[:,0], xy[:,1]\n",
    "            plt.scatter(x, y, c=col, edgecolors='k',  s=20, linewidths=1.1)\n",
    "\n",
    "        if show_non_core:\n",
    "            xy = X[class_member_mask & ~core_samples_mask]\n",
    "            x, y = xy[:,0], xy[:,1]\n",
    "            plt.scatter(x, y, c=col, s=20, linewidths=1.1)\n",
    "\n",
    "    plt.title('Estimated number of clusters: %d' % n_clusters_);\n",
    "    \n",
    "    return dbscan, unique_labels \n",
    "\n",
    "# dbscan_model, dbscan_ulabels = dbscan_clustering(standardized_data, \n",
    "#                                                        eps=0.1, \n",
    "#                                                        minpoints=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Shift Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_shift_clustering(X, quantile=0.2, n_samples=500):\n",
    "    \n",
    "    # automatically detects bandwidth \n",
    "    bandwidth = estimate_bandwidth(X, quantile=quantile, n_samples=n_samples)\n",
    "\n",
    "    ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    ms.fit(X)\n",
    "    labels = ms.labels_\n",
    "    cluster_centers = ms.cluster_centers_\n",
    "\n",
    "    labels_unique = np.unique(labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "\n",
    "    print(\"number of estimated clusters : %d\" % n_clusters_)\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.figure(dpi=200)\n",
    "    colors = cycle('byrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "    \n",
    "    for k, col in zip(range(n_clusters_), colors):\n",
    "        my_members = labels == k\n",
    "        cluster_center = cluster_centers[k]\n",
    "        plt.plot(X[my_members, 0], X[my_members, 1], col + '.')\n",
    "        plt.plot(cluster_center[0], cluster_center[1], 'X', markerfacecolor='k',\n",
    "                 markeredgecolor='k', markersize=14)\n",
    "        \n",
    "    plt.title('Estimated number of clusters: %d' % n_clusters_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering (Agglomerative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_agglomerative_clustering(X, clusters=3):\n",
    "    for linkage in ('ward', 'average', 'complete'):\n",
    "        agglo = AgglomerativeClustering(linkage=linkage, n_clusters=clusters)\n",
    "        t0 = time()\n",
    "        clusters = agglo.fit(X)\n",
    "        print(\"%s : %.2fs\" % (linkage, time() - t0))\n",
    "\n",
    "        x,y = zip(*X)\n",
    "        plt.figure(dpi=200)\n",
    "        plt.scatter(x,y,c=plt.cm.rainbow(agglo.labels_*20),s=14)\n",
    "        plt.title(\"Linkage Type: %s\" % linkage)\n",
    "        \n",
    "        return agglo, clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dendrograms with Agglomerative Clustering in SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xdata, _ = make_blobs(n_samples=50, centers=centers, cluster_std=0.6)\n",
    "Z = linkage(Xdata,'ward')\n",
    "plt.figure(dpi=200)\n",
    "dendrogram(Z,truncate_mode='mlab',); ### Dendrograms with Agglomerative Clustering in SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spectral_clustering(X, clusters=3):\n",
    "    sc = SpectralClustering(n_clusters=clusters)\n",
    "    clusters = sc.fit_predict(X)\n",
    "    \n",
    "    x,y = zip(*X)\n",
    "    plt.figure(dpi=200)\n",
    "    plt.scatter(X[:,0],X[:,1],c=plt.cm.rainbow(ypred*20),s=14);\n",
    "    \n",
    "    return clusters, sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmailAutoClustering():\n",
    "    \"\"\" Take in a corpus of emails (accepts pandas series), \n",
    "    preprocesses (cleans & tokenizes) and vectorizes them, and \n",
    "    conducts topic modeling to cluster each email into its \n",
    "    corresponding topic.\n",
    "    \n",
    "    OUT: DataFrame containing each email and its topic. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, docs):\n",
    "        self.docs = docs\n",
    "        self.tokenizer = self.nltk_tokenizer\n",
    "        self.count_vectorizer(self.docs, self.tokenizer)\n",
    "        self.nmf_algo(self.norm_data, self.feature_names)\n",
    "        \n",
    "        \n",
    "    def nltk_tokenizer(self, doc):\n",
    "        \"\"\" Takes in a corpus of documents, cleans, and tokenizes:\n",
    "        1. Remove numbers, punctuations and special charaters\n",
    "        2. Tokenize into words using wordpunct\n",
    "        3. Lemmatize\n",
    "        4. Stem and lowercase\n",
    "        5. Remove stop words\n",
    "\n",
    "        OUT: cleaned text (tokens).\n",
    "        \"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        stop = nltk.corpus.stopwords.words('english')\n",
    "        stop += ['.',',','(', ')',\"'\",'\"']\n",
    "        stop += ['_____________________________________________',\n",
    "                 '__________________________________________________']\n",
    "        stop += ['_________________________________________________________________']\n",
    "        stop = set(stop) \n",
    "        \n",
    "        doc = doc.lower()\n",
    "        doc = doc.translate(str.maketrans('','','1234567890')) \n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        tokens = [lemmatizer.lemmatize(i) for i in tokens]\n",
    "        tokens = [i for i in tokens if i not in stop]\n",
    "        tokens = [i for i in tokens if len(i)>3]\n",
    "        \n",
    "        return tokens \n",
    "             \n",
    "    def count_vectorizer(self, docs, tokenizer, min_n=1, max_n=2, max_features=5000, max_df=0.6):\n",
    "        \"\"\" This function takes in cleaned tokens and \n",
    "        returns vectorized and normalized data using count vectorizer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.vectorizer = CountVectorizer(tokenizer=tokenizer,\n",
    "                                     ngram_range=(min_n,max_n),\n",
    "                                     max_features=max_features,\n",
    "                                     max_df=max_df) \n",
    "        \n",
    "        self.vect_data = self.vectorizer.fit_transform(self.docs)\n",
    "        self.norm_data = Normalizer().fit_transform(self.vect_data)\n",
    "        self.feature_names = self.vectorizer.get_feature_names()\n",
    "        \n",
    "        \n",
    "    def nmf_algo(self, norm_data, feature_names, n_comp=4, random_state=7777, no_top_words=20, topic_names=None):\n",
    "        \"\"\" Returns a 1). NMF model and 2). transformed data\n",
    "        given the parameters specified by user.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.nmf = NMF(n_components=n_comp)\n",
    "        \n",
    "        self.nmf_data = self.nmf.fit_transform(self.norm_data)\n",
    "        \n",
    "        for ix, topic in enumerate(self.nmf.components_):\n",
    "            if not topic_names or not topic_names[ix]:\n",
    "                print(\"\\nTopic \", ix)\n",
    "            else:\n",
    "                print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "            print(\", \".join([self.feature_names[i]\n",
    "                            for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "            \n",
    "    def create_topic_space(self):\n",
    "        \"\"\" Returns a pandas dataframe with emails as row and topics & content as column \"\"\"\n",
    "        self.df_topic = pd.DataFrame(self.nmf_data)\n",
    "        self.df_topic['topics'] = self.df_topic.idxmax(axis=1)\n",
    "        df_topics = pd.get_dummies(self.df_topic['topics'])\n",
    "        self.df_topic = pd.concat([self.df_topic, df_topics, self.docs.to_frame()], axis=1)\n",
    "        self.df_topic.columns = ['t0_vec', 't1_vec','t2_vec','t3_vec', 'topics',\n",
    "                                 'Email_Bucket_1', 'Email_Bucket_2','Email_Bucket_3',\n",
    "                                 'Email_Bucket_4','Email']\n",
    "        self.df_topic = self.df_topic.drop(['t0_vec', 't1_vec','t2_vec','t3_vec','topics'], axis=1)\n",
    "        \n",
    "        return self.df_topic\n",
    "    \n",
    "#         email_topic = defaultdict(list)\n",
    "\n",
    "#         for topic, email in zip(df_topic['topics'][66:88],df_topic['Email'][66:88]):\n",
    "#             if topic==0:\n",
    "#                 email_topic['Corporation_Related'].append(email)\n",
    "#             elif topic == 1: \n",
    "#                 email_topic['Meeting_Call_Appointment'].append(email)\n",
    "#             elif topic == 2: \n",
    "#                 email_topic['IT_Related'].append(email)\n",
    "#             else: \n",
    "#                 email_topic['Industry_Business_Market'].append(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nlp_stuff(raw_string, tokenizer_, vectorizer_, topic_modeling_, \n",
    "              min_n=1, max_n=2, max_features=5000, max_df=0.6, ):\n",
    "    \"\"\"This function takes in raw text, preprocesses and vectorizes it, \n",
    "    then applies an unsurpervised learning algorithm on it.\n",
    "    \"\"\"\n",
    "    \n",
    "    if tokenizer_ == 'nltk':\n",
    "        pass \n",
    "#         tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#         stop = nltk.corpus.stopwords.words('english')\n",
    "#         stop += ['.',',','(', ')',\"'\",'\"']\n",
    "#         stop = set(stop) \n",
    "\n",
    "#         raw_string = raw_string.lower()\n",
    "#         raw_string = raw_string.translate(str.maketrans('','','1234567890'))\n",
    "#         tokens = tokenizer.tokenize(raw_string)\n",
    "#         tokens = [lemmatizer.lemmatize(i) for i in tokens]\n",
    "#         tokens = [stemmer.stem(i) for i in tokens]\n",
    "#         tokens = [i for i in tokens if i not in stop]\n",
    "#         tokens = [i for i in tokens if len(i)>3]\n",
    "    \n",
    "    \n",
    "    if tokenizer_ == 'spacy':\n",
    "        pass\n",
    "#         punctuations = string.punctuation + '.doc'\n",
    "#         raw_string = re.sub(r'\\-\\-+', '', raw_string)\n",
    "#         raw_string = re.sub(r'\\.\\.+', '', raw_string)\n",
    "#         raw_string = raw_string.translate(str.maketrans('','','1234567890'))\n",
    "#         tokens = nlp(raw_string)\n",
    "#         tokens = [tok.lemma_.lower().strip() \n",
    "#                   if tok.lemma_ != \"-PRON-\" \n",
    "#                   else tok.lower_ for tok in tokens]\n",
    "#         tokens = [tok for tok in tokens \n",
    "#                   if (tok not in stopwords and tok not in punctuations)]   \n",
    "#         tokens = [stemmer.stem(i) for i in tokens]\n",
    "#         tokens = [i for i in tokens if len(i)>3]\n",
    "\n",
    "\n",
    "    if vectorizer_ == 'countv':\n",
    "        pass \n",
    "#         vectorizer = CountVectorizer(tokenizer=tokenizer_,\n",
    "#                                  ngram_range=(min_n,max_n),\n",
    "#                                  stop_words='english', \n",
    "#                                  token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "#                                  lowercase=True,\n",
    "#                                  max_features=max_features,\n",
    "#                                  max_df = max_df) \n",
    "        \n",
    "#         vect_data = vectorizer.fit_transform(data)\n",
    "    \n",
    "    if vectorizer_ == 'tfidfv':\n",
    "        pass\n",
    "#         vectorizer = TfidfVectorizer(tokenizer=tokenizer_,\n",
    "#                                  ngram_range=(min_n,max_n),  \n",
    "#                                  stop_words='english', \n",
    "#                                  token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "#                                  lowercase=True,\n",
    "#                                  max_features=max_features,\n",
    "#                                  max_df = 0.6)\n",
    "        \n",
    "#         vect_data = vectorizer.fit_transform(data)\n",
    "        \n",
    "    if topic_modeling_ == 'isa':\n",
    "        pass \n",
    "    if topic_modeling_ == 'ida':\n",
    "        pass \n",
    "    if topic_modeling_ == 'nmf':\n",
    "        pass \n",
    "    if dim_red_ == 'pca':\n",
    "        pass \n",
    "    if clustering_ == 'kmeans':\n",
    "        pass \n",
    "    if clustering_ == 'spectral':\n",
    "        pass \n",
    "    if clustering_ == 'agglo':\n",
    "        pass \n",
    "    if clustering_ == 'meanshift':\n",
    "        pass \n",
    "    if clustering_ == 'dbscan':\n",
    "        pass "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
